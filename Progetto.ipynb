{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Achille1912/Achille1912/blob/main/Progetto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVavBXF_zTMb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66b53170-9c86-4777-af52-1e0cba458bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.28.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.65.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (60.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.4)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations) (4.11.0.86)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n",
            "Requirement already satisfied: openmim in /usr/local/lib/python3.11/dist-packages (0.3.9)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from openmim) (8.2.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from openmim) (0.4.6)\n",
            "Requirement already satisfied: model-index in /usr/local/lib/python3.11/dist-packages (from openmim) (0.1.11)\n",
            "Requirement already satisfied: opendatalab in /usr/local/lib/python3.11/dist-packages (from openmim) (0.0.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from openmim) (2.2.2)\n",
            "Requirement already satisfied: pip>=19.3 in /usr/local/lib/python3.11/dist-packages (from openmim) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openmim) (2.28.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from openmim) (13.4.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from openmim) (0.9.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from model-index->openmim) (6.0.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.11/dist-packages (from model-index->openmim) (3.8)\n",
            "Requirement already satisfied: ordered-set in /usr/local/lib/python3.11/dist-packages (from model-index->openmim) (4.1.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim) (3.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim) (4.65.2)\n",
            "Requirement already satisfied: openxlab in /usr/local/lib/python3.11/dist-packages (from opendatalab->openmim) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (3.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->openmim) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->openmim) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->openmim) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->openmim) (1.17.0)\n",
            "Requirement already satisfied: filelock~=3.14.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (3.14.0)\n",
            "Requirement already satisfied: oss2~=2.17.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (2.17.0)\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (24.2)\n",
            "Requirement already satisfied: setuptools~=60.2.0 in /usr/local/lib/python3.11/dist-packages (from openxlab->opendatalab->openmim) (60.2.0)\n",
            "Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python3.11/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (1.7)\n",
            "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.5)\n",
            "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in /usr/local/lib/python3.11/dist-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim) (2.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (0.10.0)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim) (2.22)\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu124/torch2.6.0/index.html\n",
            "Collecting mmdet\n",
            "  Using cached mmdet-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
            "Ignoring mmcv: markers 'extra == \"mim\"' don't match your environment\n",
            "Ignoring mmengine: markers 'extra == \"mim\"' don't match your environment\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mmdet) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mmdet) (2.0.2)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from mmdet) (2.0.8)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from mmdet) (1.15.3)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.11/dist-packages (from mmdet) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from mmdet) (1.17.0)\n",
            "Collecting terminaltables (from mmdet)\n",
            "  Using cached terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mmdet) (4.65.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mmdet) (2.9.0.post0)\n",
            "Using cached mmdet-3.3.0-py3-none-any.whl (2.2 MB)\n",
            "Using cached terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: terminaltables, mmdet\n",
            "Successfully installed mmdet-3.3.0 terminaltables-3.1.10\n",
            "Looking in links: https://download.openmmlab.com/mmcv/dist/cu124/torch2.6.0/index.html\n",
            "Collecting mmcv-full\n",
            "  Downloading mmcv-full-1.7.2.tar.gz (607 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m607.9/607.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting addict (from mmcv-full)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (24.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (11.2.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from mmcv-full) (6.0.2)\n",
            "Collecting yapf (from mmcv-full)\n",
            "  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->mmcv-full) (4.3.8)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading yapf-0.43.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: mmcv-full\n"
          ]
        }
      ],
      "source": [
        "!pip install kagglehub torchmetrics\n",
        "!pip install albumentations\n",
        "\n",
        "!pip install -U openmim\n",
        "!mim install mmdet\n",
        "!mim install mmcv-full\n",
        "!pip install torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGYI1p8jrFOA"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSjv-cNgrBxH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageEnhance\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "from torchmetrics.detection import MeanAveragePrecision\n",
        "import kagglehub\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjwO3serPdK"
      },
      "source": [
        "## CONFIG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ_vIK66rRM2"
      },
      "outputs": [],
      "source": [
        "# ==== CONFIG ====\n",
        "class Config:\n",
        "    path = kagglehub.dataset_download(\"orvile/p-vivax-malaria-infected-human-blood-smears\")\n",
        "    BASE_DIR = path + \"/malaria/\"\n",
        "    IMG_DIR = os.path.join(BASE_DIR, \"images\")\n",
        "    TRAIN_JSON = os.path.join(BASE_DIR, \"training.json\")\n",
        "    TEST_JSON = os.path.join(BASE_DIR, \"test.json\")\n",
        "\n",
        "    NUM_CLASSES = 8  # 7 + background\n",
        "    MODEL_SAVE_PATH = \"malaria_detection.pth\"\n",
        "\n",
        "    #seed\n",
        "    RANDOM_SEED = 42\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    BATCH_SIZE = 4 if torch.cuda.is_available() else 2\n",
        "    NUM_EPOCHS = 5\n",
        "    LEARNING_RATE = 0.0003\n",
        "    WEIGHT_DECAY = 0.001\n",
        "    PATIENCE = 5\n",
        "\n",
        "    IMG_SIZE = (1024, 1024)\n",
        "\n",
        "    MODEL = \"REPPOINTS_RESNET50\" # FASTER_RCNN_RESNET50, FASTER_RCNN_RESNET101, FASTER_RCNN_MOBILENET, RETINANET_RESNET50\n",
        "    Loss = \"FocalLoss\" # Standard, FocalLosss\n",
        "\n",
        "    Optim = \"AdamW\" # Adam, AdamW\n",
        "    Scheduler = \"ReduceLROnPlateau\" # ReduceLROnPlateau, CosineAnnealingLR\n",
        "    val_size = 0.2\n",
        "    NUM_WORKERS = 2\n",
        "    USE_AMP = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPjz_oNzrtqy"
      },
      "source": [
        "### SETUP DEVICE AND CLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sorOizrCrwQC"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "CLASSES = [\n",
        "    \"red blood cell\", \"trophozoite\", \"ring\", \"difficult\",\n",
        "    \"schizont\", \"gametocyte\", \"leukocyte\"\n",
        "]\n",
        "CLASS_TO_IDX = {cls: i+1 for i, cls in enumerate(CLASSES)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFxygo3Or6Ic"
      },
      "source": [
        "## DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCoBZJVtr7vk"
      },
      "outputs": [],
      "source": [
        "class MalariaDataset(Dataset):\n",
        "    def __init__(self, json_path, img_dir, transform=None, resize=True):\n",
        "        with open(json_path) as f:\n",
        "            self.data = json.load(f)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.resize = resize\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.data[idx]\n",
        "\n",
        "        img_name = os.path.basename(entry['image'].get('pathname',\n",
        "                          entry['image'].get('filename',\n",
        "                          entry['image'].get('id', ''))))\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Carica e migliora il contrasto dell'immagine\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = ImageEnhance.Contrast(img).enhance(1.5)\n",
        "        orig_width, orig_height = img.size\n",
        "\n",
        "        # Bounding box e label\n",
        "        boxes, labels = [], []\n",
        "\n",
        "        for obj in entry.get('objects', []):\n",
        "            if obj['category'] not in CLASS_TO_IDX:\n",
        "                continue\n",
        "            bbox = obj['bounding_box']\n",
        "            xmin = bbox['minimum']['c']\n",
        "            ymin = bbox['minimum']['r']\n",
        "            xmax = bbox['maximum']['c']\n",
        "            ymax = bbox['maximum']['r']\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(CLASS_TO_IDX[obj['category']])\n",
        "\n",
        "        boxes = np.array(boxes)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Converti PIL in NumPy per Albumentations\n",
        "        img_np = np.array(img)\n",
        "\n",
        "        # Se serve, ridimensiona manualmente prima della transform (opzionale, pu√≤ anche stare dentro le tfm)\n",
        "        if self.resize and self.transform is None:\n",
        "            scale_x = Config.IMG_SIZE[0] / orig_width\n",
        "            scale_y = Config.IMG_SIZE[1] / orig_height\n",
        "            img_np = np.array(img.resize(Config.IMG_SIZE))\n",
        "            boxes = [[xmin * scale_x, ymin * scale_y, xmax * scale_x, ymax * scale_y]\n",
        "                     for xmin, ymin, xmax, ymax in boxes]\n",
        "\n",
        "        # Applica le transformazioni (Albumentations)\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=img_np, bboxes=boxes, labels=labels)\n",
        "            img_tensor = transformed['image']\n",
        "            boxes = torch.tensor(transformed['bboxes'], dtype=torch.float32)\n",
        "            labels = torch.tensor(transformed['labels'], dtype=torch.int64)\n",
        "        else:\n",
        "            # fallback se non c'√® albumentations\n",
        "            img_tensor = transforms.ToTensor()(img)\n",
        "            img_tensor = transforms.Normalize(mean=[0.7205, 0.7203, 0.7649],\n",
        "                                              std=[0.2195, 0.2277, 0.1588])(img_tensor)\n",
        "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "            labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "        return img_tensor, target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-r2cP2qtv3q"
      },
      "source": [
        "## TRANSFORMATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import torch\n",
        "\n",
        "# from torchvision import transforms\n",
        "\n",
        "# # === CONFIG ===\n",
        "# IMG_DIR = Config.IMG_DIR\n",
        "\n",
        "# # === Trasformazione base ===\n",
        "# to_tensor = transforms.ToTensor()  # converte a [C, H, W] in [0, 1]\n",
        "\n",
        "# # === Liste per accumulare pixel per canale ===\n",
        "# mean = torch.zeros(3)\n",
        "# std = torch.zeros(3)\n",
        "# n_images = 0\n",
        "\n",
        "# print(\"Inizio calcolo media/std da immagini...\")\n",
        "\n",
        "# for fname in tqdm(os.listdir(IMG_DIR)):\n",
        "#     if not fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
        "#         continue  # ignora file non immagine\n",
        "\n",
        "#     img_path = os.path.join(IMG_DIR, fname)\n",
        "#     img = Image.open(img_path).convert(\"RGB\")\n",
        "#     tensor = to_tensor(img)  # shape: [3, H, W]\n",
        "\n",
        "#     mean += tensor.mean(dim=(1, 2))\n",
        "#     std += tensor.std(dim=(1, 2))\n",
        "#     n_images += 1\n",
        "\n",
        "# mean /= n_images\n",
        "# std /= n_images\n",
        "\n",
        "# print(f\"\\nMedia RGB: {mean}\")\n",
        "# print(f\"Deviazione standard RGB: {std}\")\n"
      ],
      "metadata": {
        "id": "FEDn908HGbhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg9CWDXitx3T"
      },
      "outputs": [],
      "source": [
        "# ==== TRASFORMAZIONI ====\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "def get_transforms(train=True):\n",
        "    if train:\n",
        "        return A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            #A.Rotate(limit=5, border_mode=1, p=0.3),\n",
        "            #A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
        "            A.CLAHE(clip_limit=1.5, p=0.3),\n",
        "            A.Resize(*Config.IMG_SIZE),\n",
        "            A.Normalize(mean=[0.7205, 0.7203, 0.7649],\n",
        "                        std=[0.2195, 0.2277, 0.1588]),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(*Config.IMG_SIZE),\n",
        "            A.Normalize(mean=[0.7205, 0.7203, 0.7649],\n",
        "                        std=[0.2195, 0.2277, 0.1588]),\n",
        "            ToTensorV2()\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYX3hW8nt-py"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Focal Loss"
      ],
      "metadata": {
        "id": "vHLwovhHeKvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.detection.roi_heads import fastrcnn_loss as original_fastrcnn_loss\n",
        "\n",
        "# Focal Loss\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=4.0, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        return focal_loss\n",
        "\n",
        "# Custom fastrcnn_loss with focal loss\n",
        "def custom_fastrcnn_loss(class_logits, box_regression, labels, regression_targets):\n",
        "    # Focal loss for classification\n",
        "    classification_loss = FocalLoss()(class_logits, labels)\n",
        "\n",
        "    # Smooth L1 loss for bounding box regression (unchanged)\n",
        "    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n",
        "    box_loss = F.smooth_l1_loss(\n",
        "        box_regression[sampled_pos_inds_subset],\n",
        "        regression_targets[sampled_pos_inds_subset],\n",
        "        beta=1.0, reduction='sum'\n",
        "    ) / max(1, labels.numel())\n",
        "\n",
        "    return classification_loss, box_loss\n"
      ],
      "metadata": {
        "id": "jpiVxe0AeM12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orn7kSBCt9_D"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import (\n",
        "    fasterrcnn_resnet50_fpn,\n",
        "    fasterrcnn_mobilenet_v3_large_fpn,\n",
        "    FasterRCNN,\n",
        "    retinanet_resnet50_fpn\n",
        ")\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models import resnet101\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
        "from torchvision.models import resnext101_32x8d\n",
        "\n",
        "\n",
        "\n",
        "# Focal loss patch (solo per Faster R-CNN)\n",
        "def replace_fastrcnn_loss_with_focal(model):\n",
        "    def custom_forward(self, class_logits, box_regression, labels, regression_targets):\n",
        "        return custom_fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n",
        "    model.roi_heads.fastrcnn_loss = custom_forward.__get__(model.roi_heads, type(model.roi_heads))\n",
        "\n",
        "\n",
        "# Costruisce ResNet101 con FPN\n",
        "def build_resnet101_backbone():\n",
        "    backbone = resnet101(weights=\"DEFAULT\")\n",
        "    return_layers = {\n",
        "        'layer1': '0',\n",
        "        'layer2': '1',\n",
        "        'layer3': '2',\n",
        "        'layer4': '3',\n",
        "    }\n",
        "    in_channels_stage2 = 256\n",
        "    in_channels_list = [\n",
        "        in_channels_stage2,\n",
        "        in_channels_stage2 * 2,\n",
        "        in_channels_stage2 * 4,\n",
        "        in_channels_stage2 * 8,\n",
        "    ]\n",
        "    out_channels = 256\n",
        "\n",
        "    body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "    backbone_with_fpn = BackboneWithFPN(\n",
        "        body=body,\n",
        "        return_layers=return_layers,\n",
        "        in_channels_list=in_channels_list,\n",
        "        out_channels=out_channels\n",
        "    )\n",
        "    return backbone_with_fpn\n",
        "\n",
        "def build_resnext101_backbone():\n",
        "    backbone = resnext101_32x8d(weights=\"DEFAULT\")\n",
        "\n",
        "    # Mappa dei layer che vogliamo usare per FPN\n",
        "    return_layers = {\n",
        "        'layer1': '0',\n",
        "        'layer2': '1',\n",
        "        'layer3': '2',\n",
        "        'layer4': '3',\n",
        "    }\n",
        "\n",
        "    in_channels_stage2 = 256  # layer1 output\n",
        "    in_channels_list = [\n",
        "        in_channels_stage2,        # layer1\n",
        "        in_channels_stage2 * 2,    # layer2\n",
        "        in_channels_stage2 * 4,    # layer3\n",
        "        in_channels_stage2 * 8,    # layer4\n",
        "    ]\n",
        "    out_channels = 256\n",
        "\n",
        "    # Costruzione corpo troncato\n",
        "    body = IntermediateLayerGetter(backbone, return_layers=return_layers)\n",
        "\n",
        "    # Backbone con FPN\n",
        "    backbone_with_fpn = BackboneWithFPN(\n",
        "        body,\n",
        "        return_layers=return_layers,\n",
        "        in_channels_list=in_channels_list,\n",
        "        out_channels=out_channels\n",
        "    )\n",
        "    return backbone_with_fpn\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Crea modello detection con backbone configurabile\n",
        "def create_model(num_classes=2):\n",
        "    if Config.MODEL == \"FASTER_RCNN_RESNET50\":\n",
        "        model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    elif Config.MODEL == \"FASTER_RCNN_RESNET101\":\n",
        "        backbone = build_resnet101_backbone()\n",
        "        model = FasterRCNN(backbone=backbone, num_classes=num_classes)\n",
        "\n",
        "    elif Config.MODEL == \"FASTER_RCNN_MOBILENET\":\n",
        "        model = fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    elif Config.MODEL == \"RETINANET_RESNET50\":\n",
        "        model = retinanet_resnet50_fpn(weights=\"DEFAULT\", )\n",
        "        in_features = model.head.classification_head.cls_logits.in_channels\n",
        "        num_anchors = model.head.classification_head.num_anchors\n",
        "        model.head.classification_head.cls_logits = torch.nn.Conv2d(\n",
        "            in_features, num_anchors * num_classes, kernel_size=3, stride=1, padding=1\n",
        "        )\n",
        "        model.head.classification_head.num_classes = num_classes\n",
        "        return model\n",
        "    elif Config.MODEL == \"FASTER_RCNN_RESNEXT101\":\n",
        "        backbone = build_resnext101_backbone()\n",
        "        model = FasterRCNN(backbone=backbone, num_classes=num_classes)\n",
        "    elif Config.MODEL == \"REPPOINTS_RESNET50\":\n",
        "        from mmdet.apis import init_detector\n",
        "        config_url = \"https://raw.githubusercontent.com/open-mmlab/mmdetection/master/configs/reppoints/reppoints_moment_r50_fpn_1x_coco.py\"\n",
        "        checkpoint_url = \"https://download.openmmlab.com/mmdetection/v2.0/reppoints/reppoints_moment_r50_fpn_1x_coco/reppoints_moment_r50_fpn_1x_coco_20200329-4f0f7cf9.pth\"\n",
        "        config_path = \"configs/reppoints/reppoints_moment_r50_fpn_1x_coco.py\"\n",
        "        checkpoint_path = \"reppoints_r50_fpn_1x.pth\"\n",
        "\n",
        "        if not os.path.exists(config_path):\n",
        "            os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "            urllib.request.urlretrieve(config_url, config_path)\n",
        "        if not os.path.exists(checkpoint_path):\n",
        "            urllib.request.urlretrieve(checkpoint_url, checkpoint_path)\n",
        "\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        model = init_detector(config_path, checkpoint_path, device=device)\n",
        "        return model\n",
        "    else:\n",
        "        raise ValueError(f\"Backbone '{Config.MODEL}' non supportato.\")\n",
        "\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "    if Config.Loss == \"FocalLoss\":\n",
        "        replace_fastrcnn_loss_with_focal(model)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4WoY_jIuOy2"
      },
      "source": [
        "## TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suHMNO1ZuQIV"
      },
      "outputs": [],
      "source": [
        "def train_model():\n",
        "    model = create_model().to(device)\n",
        "    # OPTIMIZER\n",
        "    if Config.Optim == \"AdamW\":\n",
        "      optimizer = torch.optim.AdamW(\n",
        "          [p for p in model.parameters() if p.requires_grad],\n",
        "          lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
        "    elif Config.Optim == \"Adam\":\n",
        "      optimizer = torch.optim.Adam(\n",
        "          [p for p in model.parameters() if p.requires_grad],\n",
        "          lr=Config.LEARNING_RATE, weight_decay=Config.WEIGHT_DECAY)\n",
        "    # LR SCHEDULER\n",
        "    if Config.Scheduler == \"ReduceLROnPlateau\":\n",
        "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "          optimizer, mode='min', patience=2, factor=0.5)\n",
        "    elif Config.Scheduler == \"CosineAnnealingLR\":\n",
        "      scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "          optimizer, T_max=Config.NUM_EPOCHS, eta_min=1e-6)\n",
        "    dataset = MalariaDataset(Config.TRAIN_JSON, Config.IMG_DIR,\n",
        "                             transform=get_transforms(train=True), resize=False)\n",
        "\n",
        "    # Split dataset into training and validation sets\n",
        "    val_size = int(Config.val_size * len(dataset))\n",
        "    train_size = len(dataset) - val_size\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(Config.RANDOM_SEED)\n",
        "    )\n",
        "\n",
        "    # Create DataLoaders for both sets\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                              shuffle=True, num_workers=Config.NUM_WORKERS,\n",
        "                              collate_fn=lambda x: tuple(zip(*x)))\n",
        "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=Config.NUM_WORKERS,\n",
        "                            collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        # --- Training phase ---\n",
        "        model.train()\n",
        "        epoch_train_loss = 0.0\n",
        "\n",
        "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            for t in targets:\n",
        "                t['labels'] = torch.ones_like(t['labels'])  # Remove this line if your dataset has real labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast('cuda', enabled=Config.USE_AMP):\n",
        "                loss_dict = model(images, targets)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_train_loss += losses.item()\n",
        "\n",
        "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        # --- Validation phase ---\n",
        "        print(\"\\nStart Validation... üîç\")\n",
        "        epoch_val_loss = 0.0\n",
        "\n",
        "        # Force model.train() to get loss dict, but still disable gradients\n",
        "        model.train()\n",
        "        with torch.no_grad():\n",
        "            for images, targets in val_loader:\n",
        "                images = [img.to(device) for img in images]\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "                for t in targets:\n",
        "                    t['labels'] = torch.ones_like(t['labels'])  # Remove if not needed\n",
        "\n",
        "                with torch.amp.autocast('cuda', enabled=Config.USE_AMP):\n",
        "                    loss_dict = model(images, targets)\n",
        "\n",
        "                    # Ensure it's a dict and not a list of predictions\n",
        "                    if isinstance(loss_dict, dict):\n",
        "                        losses = sum(loss for loss in loss_dict.values())\n",
        "                        epoch_val_loss += losses.item()\n",
        "                    else:\n",
        "                        raise ValueError(\"Model returned predictions instead of a loss dict during validation. Check model mode or target input.\")\n",
        "\n",
        "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
        "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "                'best_loss': best_loss,\n",
        "            }, Config.MODEL_SAVE_PATH)\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= Config.PATIENCE:\n",
        "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaYXewYOB-"
      },
      "source": [
        "## TEST AND VISUALIZZATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BgjXS_EYeu-"
      },
      "outputs": [],
      "source": [
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "from sklearn.metrics import average_precision_score\n",
        "import matplotlib.patches as patches\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class NpEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, (np.integer,)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.floating,)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, (np.ndarray,)):\n",
        "            return obj.tolist()\n",
        "        return super().default(obj)\n",
        "\n",
        "def calculate_iou(box1, box2):\n",
        "    x1, y1 = max(box1[0], box2[0]), max(box1[1], box2[1])\n",
        "    x2, y2 = min(box1[2], box2[2]), min(box1[3], box2[3])\n",
        "    inter = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union = area1 + area2 - inter\n",
        "    return inter / union if union > 0 else 0.0\n",
        "\n",
        "def compute_mean_iou_per_gt(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
        "    if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
        "        return 0.0, []\n",
        "\n",
        "    iou_matrix = [[calculate_iou(pb, gt) for gt in gt_boxes] for pb in pred_boxes]\n",
        "    matched_pairs = []\n",
        "    used_preds = set()\n",
        "\n",
        "    for gt_idx in range(len(gt_boxes)):\n",
        "        best_iou = 0.0\n",
        "        best_pred_idx = -1\n",
        "        for pred_idx in range(len(pred_boxes)):\n",
        "            if pred_idx in used_preds:\n",
        "                continue\n",
        "            iou = iou_matrix[pred_idx][gt_idx]\n",
        "            if iou > best_iou:\n",
        "                best_iou = iou\n",
        "                best_pred_idx = pred_idx\n",
        "        if best_iou >= iou_threshold:\n",
        "            matched_pairs.append((best_pred_idx, gt_idx, best_iou))\n",
        "            used_preds.add(best_pred_idx)\n",
        "\n",
        "    mean_iou = np.mean([iou for _, _, iou in matched_pairs]) if matched_pairs else 0.0\n",
        "    return mean_iou, matched_pairs\n",
        "\n",
        "def evaluate_test_set(model, dataset, score_threshold=0.5, save_path=None):\n",
        "    model.eval()\n",
        "    total_iou = 0.0\n",
        "    num_samples = 0\n",
        "    all_ious = []\n",
        "    total_tp = total_fp = total_fn = 0\n",
        "\n",
        "    for img, target in dataset:\n",
        "        with torch.no_grad():\n",
        "            prediction = model([img.to(device)])[0]\n",
        "\n",
        "        pred_boxes = prediction['boxes'].cpu().numpy()\n",
        "        pred_scores = prediction['scores'].cpu().numpy()\n",
        "        gt_boxes = target['boxes'].cpu().numpy()\n",
        "\n",
        "        keep = pred_scores >= score_threshold\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "\n",
        "        mean_iou, matched_pairs = compute_mean_iou_per_gt(pred_boxes, gt_boxes)\n",
        "        precision, recall, f1_score, tp, fp, fn = compute_precision_recall_f1(pred_boxes, gt_boxes)\n",
        "\n",
        "        if len(gt_boxes) > 0:\n",
        "            total_iou += mean_iou\n",
        "            num_samples += 1\n",
        "            all_ious.extend([iou for _, _, iou in matched_pairs])\n",
        "            total_tp += tp\n",
        "            total_fp += fp\n",
        "            total_fn += fn\n",
        "\n",
        "    mean_iou_all = total_iou / num_samples if num_samples > 0 else 0.0\n",
        "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "    recall    = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "    f1_score  = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    results = {\n",
        "        'miou': mean_iou_all,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1_score,\n",
        "        'tp': total_tp,\n",
        "        'fp': total_fp,\n",
        "        'fn': total_fn\n",
        "    }\n",
        "\n",
        "    print(f\"\\n=== [Test Set Evaluation] ===\")\n",
        "    print(f\"üìä Metriche globali:\")\n",
        "    print(f\"  - mIoU     : {results['miou']:.4f}\")\n",
        "    print(f\"  - Precision: {results['precision']:.4f}\")\n",
        "    print(f\"  - Recall   : {results['recall']:.4f}\")\n",
        "    print(f\"  - F1 Score : {results['f1']:.4f}\")\n",
        "\n",
        "    print(f\"\\nüî¢ Conteggi:\")\n",
        "    print(f\"  - TP: {results['tp']} | FP: {results['fp']} | FN: {results['fn']}\")\n",
        "\n",
        "    if all_ious:\n",
        "        print(f\"\\nüìà IoU stats:\")\n",
        "        print(f\"  - min   : {min(all_ious):.2f}\")\n",
        "        print(f\"  - max   : {max(all_ious):.2f}\")\n",
        "        print(f\"  - median: {np.median(all_ious):.2f}\")\n",
        "    else:\n",
        "        print(\"\\nüìâ Nessun IoU calcolabile.\")\n",
        "\n",
        "\n",
        "    if save_path:\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(results, f, indent=2, cls=NpEncoder)\n",
        "        print(f\"Results saved to {save_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "def compute_precision_recall_f1(pred_boxes, gt_boxes, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calcola TP, FP, FN, Precision, Recall, F1 Score per una singola immagine.\n",
        "    \"\"\"\n",
        "    matched_gt = set()\n",
        "    tp = 0\n",
        "\n",
        "    for pb in pred_boxes:\n",
        "        match_found = False\n",
        "        for i, gb in enumerate(gt_boxes):\n",
        "            if i in matched_gt:\n",
        "                continue\n",
        "            iou = calculate_iou(pb, gb)\n",
        "            if iou >= iou_threshold:\n",
        "                tp += 1\n",
        "                matched_gt.add(i)\n",
        "                match_found = True\n",
        "                break\n",
        "    fp = len(pred_boxes) - tp\n",
        "    fn = len(gt_boxes) - tp\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1_score  = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    return precision, recall, f1_score, tp, fp, fn\n",
        "\n",
        "\n",
        "def compute_map(model, dataset, score_threshold=0.5, save_path=None):\n",
        "    model.eval()\n",
        "    metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
        "    preds, targets = [], []\n",
        "\n",
        "    for img, target in dataset:\n",
        "        with torch.no_grad():\n",
        "            pred = model([img.to(device)])[0]\n",
        "\n",
        "        scores = pred['scores'].cpu()\n",
        "        boxes = pred['boxes'].cpu()\n",
        "        labels = pred['labels'].cpu()\n",
        "        keep = scores >= score_threshold\n",
        "\n",
        "        preds_dict = {\n",
        "            \"boxes\": boxes[keep],\n",
        "            \"scores\": scores[keep],\n",
        "            \"labels\": labels[keep]\n",
        "        }\n",
        "\n",
        "        target_dict = {\n",
        "            \"boxes\": target['boxes'].cpu(),\n",
        "            \"labels\": target['labels'].cpu()\n",
        "        }\n",
        "\n",
        "        preds.append(preds_dict)\n",
        "        targets.append(target_dict)\n",
        "\n",
        "    metric.update(preds, targets)\n",
        "    results = metric.compute()\n",
        "\n",
        "    print(f\"\\n=== [mAP Evaluation] ===\")\n",
        "    for k, v in results.items():\n",
        "        if torch.is_tensor(v):\n",
        "            v = v.item() if v.numel() == 1 else v.tolist()\n",
        "        if isinstance(v, float):\n",
        "            print(f\"  - {k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  - {k}: {v}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if save_path:\n",
        "        with open(save_path, 'w') as f:\n",
        "            json.dump(results, f, indent=2, cls=NpEncoder)\n",
        "        print(f\"mAP results saved to {save_path}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results_to_file(test_results, map_results, out_path=\"results\"):\n",
        "    os.makedirs(out_path, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filepath = os.path.join(out_path, f\"results_{timestamp}.json\")\n",
        "\n",
        "    def convert(o):\n",
        "        if isinstance(o, torch.Tensor):\n",
        "            return o.item() if o.numel() == 1 else o.tolist()\n",
        "        elif isinstance(o, np.ndarray):\n",
        "            return o.tolist()\n",
        "        elif isinstance(o, (np.float32, np.float64)):\n",
        "            return float(o)\n",
        "        elif isinstance(o, (np.int32, np.int64)):\n",
        "            return int(o)\n",
        "        return str(o)\n",
        "\n",
        "    # Serializza la classe Config\n",
        "    config_dict = {\n",
        "        key: convert(val) for key, val in vars(Config).items()\n",
        "        if not key.startswith(\"__\") and not callable(val)\n",
        "    }\n",
        "\n",
        "    combined_results = {\n",
        "        \"config\": config_dict,\n",
        "        \"test_metrics\": json.loads(json.dumps(test_results, default=convert)),\n",
        "        \"map_metrics\": json.loads(json.dumps(map_results, default=convert)),\n",
        "    }\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(combined_results, f, indent=4)\n",
        "\n",
        "    print(f\"üìÅ Risultati salvati in: {filepath}\")\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        files.download(filepath)\n",
        "        print(\"‚¨áÔ∏è File pronto per il download!\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è Non sei in Colab, download automatico non disponibile.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize_results(model_path, dataset, num_samples=3, score_threshold=0.5):\n",
        "    model = create_model().to(device)\n",
        "    checkpoint = torch.load(model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    test_results = evaluate_test_set(model, dataset, score_threshold)\n",
        "    map_results = compute_map(model, dataset, score_threshold)\n",
        "\n",
        "    save_results_to_file(test_results, map_results)\n",
        "\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        img, target = dataset[i]\n",
        "        with torch.no_grad():\n",
        "            pred = model([img.to(device)])[0]\n",
        "\n",
        "        img_disp = img.cpu().clone()\n",
        "        img_disp = img_disp * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "        img_disp = img_disp + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "        img_disp = img_disp.clamp(0, 1).permute(1, 2, 0).numpy()\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(img_disp)\n",
        "        ax = plt.gca()\n",
        "\n",
        "        gt_boxes = target['boxes'].cpu().numpy()\n",
        "        pred_boxes = pred['boxes'].cpu().numpy()\n",
        "        pred_scores = pred['scores'].cpu().numpy()\n",
        "\n",
        "        keep = pred_scores >= score_threshold\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "        pred_scores = pred_scores[keep]\n",
        "\n",
        "        mean_iou, matched_pairs = compute_mean_iou_per_gt(pred_boxes, gt_boxes)\n",
        "        precision, recall, f1, tp, fp, fn = compute_precision_recall_f1(pred_boxes, gt_boxes)\n",
        "\n",
        "        print(f\"\\n=== Sample {i+1} ===\")\n",
        "        print(f\"GT: {len(gt_boxes)}, Predetti (score ‚â• {score_threshold}): {len(pred_boxes)}\")\n",
        "        print(f\"mIoU: {mean_iou:.4f}\")\n",
        "        print(f\"TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
        "\n",
        "        matched_gt_indices = {gt_idx for (_, gt_idx, _) in matched_pairs}\n",
        "\n",
        "        for gt_idx, box in enumerate(gt_boxes):\n",
        "            x1, y1, x2, y2 = box\n",
        "            color = 'green' if gt_idx in matched_gt_indices else 'blue'\n",
        "            ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
        "                                       fill=False, color=color, linewidth=2))\n",
        "\n",
        "        for idx, (box, score) in enumerate(zip(pred_boxes, pred_scores)):\n",
        "            text = f\"{score:.2f}\"\n",
        "            for (pred_idx, gt_idx, iou) in matched_pairs:\n",
        "                if idx == pred_idx:\n",
        "                    text += f\" (IoU: {iou:.2f})\"\n",
        "                    break\n",
        "            ax.add_patch(plt.Rectangle((box[0], box[1]),\n",
        "                                       box[2] - box[0], box[3] - box[1],\n",
        "                                       fill=False, color='red', linewidth=2))\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Sample {i+1} | mIoU: {mean_iou:.2f}\")\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXA3UidlvMVM"
      },
      "source": [
        "## MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVFu8qMxvMA2"
      },
      "outputs": [],
      "source": [
        "# ==== MAIN ====\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting training... ‚è≥\")\n",
        "\n",
        "    print({\n",
        "         \"model\": Config.MODEL,\n",
        "         \"lr\": Config.LEARNING_RATE,\n",
        "         \"batch_size\": Config.BATCH_SIZE,\n",
        "        \"num_epochs\": Config.NUM_EPOCHS,\n",
        "        \"weight_decay\": Config.WEIGHT_DECAY,\n",
        "        \"patience\": Config.PATIENCE,\n",
        "         \"optimizer\": Config.Optim,\n",
        "         \"scheduler\": Config.Scheduler,\n",
        "        \"img_size\": Config.IMG_SIZE\n",
        "    })\n",
        "    history = train_model() # changed name to history\n",
        "\n",
        "    # Plot training and validation loss separately\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    print(\"\\nStarting Test and Visualize Samples...üí°\")\n",
        "    test_dataset = MalariaDataset(Config.TEST_JSON, Config.IMG_DIR,\n",
        "                                  transform=get_transforms(train=False), resize=True)\n",
        "    visualize_results(Config.MODEL_SAVE_PATH, test_dataset, 3, 0.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}